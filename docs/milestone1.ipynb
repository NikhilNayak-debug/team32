{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d2aef29",
   "metadata": {},
   "source": [
    "# CS 107 - Milestone\n",
    "\n",
    "20th October, 2022\n",
    "\n",
    "Team: Kareema Batool, Nikhil Nayak, Nishtha Sardana, Saket Joshi, Sree Harsha Tanneru\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Differentiation is defined as the process of finding the gradients/derivatives of a particular function in hand. It finds multiple applications in the areas of science and engineering. With the exponential growth in the size of the dataset and advancements in technologies - the complexity of computing derivatives has increased and we have become increasingly dependent on computers to compute derivatives. \n",
    "\n",
    "Currently, there are three ways to compute derivatives - finite, symbolic, automatic differentiation. The finite differentiation method although being quick and easy to implement - suffers from machine precision and rounding errors. We are able to alleviate these issues using symbolic differentiation, however, it becomes computationally very expensive as the function(s) starts to get complex. We are able to alleviate both the issues of computational complexity and machine precision using automatic differentiation.  \n",
    "\n",
    "Automatic Differentiation leverages symbolic rules for evaluating gradients - which is more accurate than using finite difference approximations. But unlike a purely symbolic process, the evaluation of expressions takes place early in the computation - it evaluates derivatives at particular numeric values. \n",
    "\n",
    "The package fab-AD implements automatic differentiation for computational use. fabAD can be used to automatically differentiate functions via forward mode. Automatic Differentiation finds applications in optimization, machine learning, and numerical methods. \n",
    "\n",
    "## 2. Background\n",
    "### 2.1 An overview of Automatic Differentiation\n",
    "\n",
    "Automatic differentiation (also known as autodiff, AD, or algorithmic differentiation) is a widely used tool in optimization. There are two ways in which we can calculate the derivative of a function. If it is  totally new function, we can use the limit definition to derive it. However, most of the time  a function is just a composition of old (seen) functions and in this case we can use chain rule to derive the new function. \n",
    "\n",
    "Automatic differentiation provides the benefit of avoiding the symbolic manipulation of functions while achieving machine-like precision. Automatic differentiation has applications in astronomy, dynamic systems, numerical analytical research, optimization in finance and engineering, among others.\n",
    "\n",
    "AD is based on the concept of decomposing a function into a series of simple operations and functions whose derivatives are readily attainable, and then sequentially applying the chain rule to assess the derivatives of these operations to calculate the derivative of the entire function. This eliminates the difficulty of parsing and memorizing the entire symbolic expression, allowing us to keep simply the function's value and its derivative at each step.\n",
    "\n",
    "The two primary strategies for automated distinction are forward mode and reverse mode. These two modes are identical in terms of precision, but they may differ in terms of efficiency when the size of the input data grows. Forward mode is often more effective when working with complicated functions or a large number of functions. Some AD algorithms even implement both forward mode and reverse mode. For this project, our package implements solely the forward mode and is a valuable resource for applications such as dynamic systems and mechanical engineering.\n",
    "\n",
    "To better comprehend automated differentiation, let's first familiarize ourselves with several essential principles employed in the AD algorithms. We will use the remainder of this section to introduce them quickly.\n",
    "\n",
    "### 2.2 Elementary operations and functions\n",
    "\n",
    "Automatic differentiation's algorithm reduces functions to simple arithmetic operations and elementary functions. The fundamental arithmetic operations are addition, subtraction, multiplication, and division (we can also consider taking roots of a number as raising it to powers less than ). Among the elementary functions are exponential, logarithmic, and trigonometric. These operations and functions have easily calculable derivates, thus we employ them as elementary evaluation stages in the AD evaluation trace.\n",
    "\n",
    "### 2.3 The chain rule\n",
    "\n",
    "The chain rule permits the decomposition of complex, nested functions into layers of operations. Our automated differentiation algorithm sequentially computes the derivative of functions using the chain rule.\n",
    "\n",
    "The chain rule can be used to calculate the derivate of nested functions, such in the form of $u(v(t))$. For this function, the derivative of $u$ with respect to $t$ is $$\\dfrac{\\partial u}{\\partial t} = \\dfrac{\\partial u}{\\partial v}\\dfrac{\\partial v}{\\partial t}.$$\n",
    "\n",
    "A more general form of chain rule applies when a function $h$ has several arguments, or when its argument is a vector. Suppose we have $h = h(y(t))$ where  $y \\in R^n$ and $t \\in R^m $. Here, $h$ is the combination of $n$ functions, each of which has $m$ variables. Using the chain rule, the derivative of $h$ with respect to $t$, now called the gradient of $h$, is\n",
    "\n",
    "$$\\nabla_{t}h = \\sum_{i=1}^{n}{\\frac{\\partial h}{\\partial y_{i}}\\nabla y_{i}\\left(t\\right)}.$$\n",
    "\n",
    "### 2.4 Evaluation Trace and Computational Graph\n",
    "Our approach for automated differentiation is founded on the concepts of evaluation trail and computational graph.\n",
    "\n",
    "The evaluation trace traces each operation layer while the input function and its derivative are evaluated. The evaluation trace contains the traces, elementary operations, numeric values, elementary derivatives, and partial derivatives at each step.\n",
    "\n",
    "The computational graph represents the evaluation path graphically. It contains the traces and elementary operations of all the stages, linking them with arrows leading from the input to the output of each step, allowing us to better comprehend the function's structure and evaluation trail. In forward mode, operations are executed from the beginning to the conclusion of a graph or evaluation trace. Reverse mode executes the processes in reverse, applying the chain rule each time to get the trace's derivative.\n",
    "For example, we have a function:\n",
    "$$f(x,y, z) = x*y + z$$\n",
    "![image1.png](image1.png)\n",
    "\n",
    "## 3. How to use fab-AD?\n",
    "\n",
    "\n",
    "We plan to distribute and make our package available as an open-source project. For easy installation, we will use the PyPI with PEP517/518, so the package will be installable through pip.\n",
    "\n",
    "\n",
    "An example installation would be \n",
    "``` pip install –-user fab-ad ```\n",
    "\n",
    "Sample usage will be\n",
    "```python3\n",
    "from fab-ad import FabNode, FabGraph\n",
    "\n",
    "fabnode_a = FabNode(value=2, derivative=1)\n",
    "fabnode_b = FabNode(value=2, derivative=1)\n",
    "fabgraph = FabGraph.get_graph(\n",
    "    fn = lambda x1, x2: x1*x2,\n",
    "    inputs=[fabnode_a, fabnode_b]\n",
    ")\n",
    "\n",
    "fabgraph.add_node(fabnode_a + fabnode_b)\n",
    "jacob = fabgraph.compute_jacobian()\n",
    "``` \n",
    "\n",
    "\n",
    "## 4. Software Organization\n",
    "\n",
    "The home directory of our software package would be structured as follows.\n",
    "\n",
    "```bash\n",
    "- LICENSE.txt\n",
    "- README.md\n",
    "- requirements.txt\n",
    "- docs/\n",
    "    * README.md\n",
    "    * milestone1.ipynb\n",
    "    * documentation.ipynb\n",
    "    * documentation.md\n",
    "    * api\n",
    "- setup.py\n",
    "- demo.ipynb\n",
    "- src/\n",
    "    - fabad/\n",
    "        * \\_\\_init\\_\\_.py\n",
    "        * AD.py\n",
    "        * AD_vec.py\n",
    "        * admath.py\n",
    "\n",
    "- tests/\n",
    "    * test_operator.py\n",
    "    * test_graph.py\n",
    "    * test_forwardmode.py\n",
    "    * test_backwardmode.py\n",
    "\n",
    "- TravisCI.yml\n",
    "- CodeCov.yml\n",
    "```\n",
    "Specificly speaking, the README file would contain a general package description and the necessary information for users to navigate in the subdirectories. Besides, we would place our documentation, testing api, and previous milestone files in the `docs` directory. Moreover, to package our model with PyPI, we need to include `setup.py` and an `fabad` directory in `src` directory, where stores the source code of our package. These core modulesinclude: `AD.py` (used by `AD_vec` to handle single value inputs), `AD_vec.py` (defines the main object class VAD and some of its related functions), and `admath.py` (defines elementary math operations for VAD objects). Furthermore, we would put a collection of test cases in `tests` directory. The tests are run through Pytest. Last but not least, we would include TravisCI.yml and CodeCov.yml in our home directory for integrated test. In addition, we also included a simple tutorial `demo.ipynb` for demo of our code in the home directory.\n",
    "\n",
    "To distribute our package, we would use PyPI so that users could easily install the package with *pip install autodiffCST*.\n",
    "For developers, the repository can be cloned by running git clone https://github.com/auto-differentiaters-in-CST/cs107-FinalProject.git from the command line.\n",
    "\n",
    "\n",
    "# Section 5: Implementation\n",
    "\n",
    "### What classes do you need and what will you implement first? What are the core data structures? How will you incorporate dual numbers? What method and name attributes will your classes have?\n",
    "\n",
    "\n",
    "`fabNode`, and `fabGraph` are the main classes in `fabAD` package. We intend to implement `fabNode` class first, followed by `fabGraph` class. `fabNode`, and `fabGraph` are the core data structures. Apart from these, we will use built-in datastructures `List`, `Dict` etc. Dual numbers can be incorportaed in `fabNode` objects, where real part correponds to `value` attribute, and imaginary part corresponds to `derivative` attribute.\n",
    "\n",
    "```python\n",
    "class fabNode(object):\n",
    "\n",
    "  def __init__(self, value, derivative, mode):\n",
    "    # real part\n",
    "    self.value = value\n",
    "    # complex part\n",
    "    self.derivative = derivative\n",
    "    self.mode = \"forward\"\n",
    "\n",
    "  # arithimetic dunder methods\n",
    "\n",
    "  def __add__(self, other):\n",
    "    raise NotImplementedError\n",
    "  def __sub__(self, other):\n",
    "    raise NotImplementedError\n",
    "  def __mul__(self, other):\n",
    "    raise NotImplementedError\n",
    "  def __div__(self, other):\n",
    "    raise NotImplementedError\n",
    "  def __pow__(self, other):\n",
    "    raise NotImplementedError\n",
    "\n",
    "  # reflected arithimetic dunder methods\n",
    "  def __radd__(self, other):\n",
    "    raise NotImplementedError\n",
    "  def __rsub__(self, other):\n",
    "    raise NotImplementedError\n",
    "  def __rmul__(self, other):\n",
    "    raise NotImplementedError\n",
    "  def __rdiv__(self, other):\n",
    "    raise NotImplementedError\n",
    "  def __rpow__(self, other):\n",
    "    raise NotImplementedError\n",
    "\n",
    "  # comparision dunder methods\n",
    "  def __eq__(self, other):\n",
    "    raise NotImplementedError\n",
    "  def __neq__(self, other):\n",
    "    raise NotImplementedError  \n",
    "  def __neg__(self, other):\n",
    "    raise NotImplementedError  \n",
    "  def __gt__(self, other):\n",
    "    raise NotImplementedError\n",
    "  def __ge__(self, other):\n",
    "    raise NotImplementedError  \n",
    "  def __lt__(self, other):\n",
    "    raise NotImplementedError \n",
    "  def __le__(self, other):\n",
    "    raise NotImplementedError\n",
    "\n",
    "  # additional dunder methods\n",
    "  def __abs__(self, other):\n",
    "    raise NotImplementedError \n",
    "  def __repr__(self, other):\n",
    "    raise NotImplementedError \n",
    "  def __str__(self, other):\n",
    "    raise NotImplementedError \n",
    "\n",
    "  def reset_gradient(self):\n",
    "    del self.derivative\n",
    "    return\n",
    "\n",
    "  def directional_derivative(self, p):\n",
    "    # computes derivative w.r.t seed vector p\n",
    "    raise NotImplementedError\n",
    "\n",
    "   @staticmethod\n",
    "   def sqrt(variable):\n",
    "   @staticmethod\n",
    "   def exp(variable):\n",
    "   @staticmethod\n",
    "   def log(variable)\n",
    "   @staticmethod\n",
    "   def sin(variable)\n",
    "   @staticmethod\n",
    "   def cos(variable)\n",
    "   @staticmethod\n",
    "   def tan(variable)\n",
    "   @staticmethod\n",
    "   def arcsin(variable)\n",
    "   @staticmethod\n",
    "   def arccos(variable)\n",
    "   @staticmethod\n",
    "   def arctan(variable)\n",
    "   @staticmethod\n",
    "   def sinh(variable)\n",
    "   @staticmethod\n",
    "   def cosh(variable)\n",
    "   @staticmethod\n",
    "   def tanh(variable)\n",
    "```\n",
    "\n",
    "```\n",
    "class fabGraph(object):\n",
    "   def compute_jocabian(self):\n",
    "\n",
    "   def primal_trace_table(self):\n",
    "\n",
    "   def tangent_trace_table(self):\n",
    "\n",
    "   @classmethod\n",
    "   def get_graph(cls, function):\n",
    "    returns cls()\n",
    "\n",
    "```\n",
    "\n",
    "`fabNode` class has `value` and `derivative` attributes to store value and gradients respectively. To support basic arithimetic, inequalities etc, we overload dunder methods listed above. `fabNode` class also has `reset_gradient` method, to reset gradients after every mini-batch of training. There's a `directional_derivative` method to compute gradients along a direction vector `p`.\n",
    "\n",
    "`fabGraph` class creates graph, and maintains jacobian, and primal, tangent trace table of the graph. Jacobian stores the derivative of the function vector with respect to the given seed vectors. Primal and tangent traces store the value of the variables and derivatives respectively for all the nodes in the graph.\n",
    "\n",
    "\n",
    "\n",
    "### Will you need some graph class to resemble the computational graph in forward mode or maybe later for reverse mode? Note that in milestone 2 you propose an extension for your project, an example could be reverse mode.\n",
    "\n",
    "Though graph class is not absolutely necessary in forward mode, since we need to store the entire graph and gradients of each node, as we compute gradient multiple times by changing direction vector. However, we expect to adapt the graph object for reverse mode automatic differentiation, by storing only the consecutive and subsequent derivatives in the trace data structure as we don't use directional derivative in this mode.\n",
    "\n",
    "### Think about how your basic operator overloading template should look like. How will you deal with elementary functions like sin, sqrt, log, and exp (and many others)?\n",
    "\n",
    "In the `fabNode` class, we overload the dunder methods for basic arithimetic and inequality operators. \n",
    "\n",
    "For `sin`, `sqrt`, `log`, and `exp` and similar operations, we will need to overload because we need to set the primal and tangent attributes of the class separately. These methods are static as they are not instance dependent and can be used as what Numpy functions would without keeping track of the derivatives. \n",
    "\n",
    "\n",
    "### If you do not want to rule out reverse mode later on, think about your operator overloading template might be improved as you will want to use dual numbers for forward mode but you may need to be able to compute $\\dfrac{\\partial v_j}{\\partial v_i}$ when performing a forward pass for reverse mode.\n",
    "\n",
    "Reverse mode automatic differentiation uses an extension of the forward mode computational graph to enable the computation of a gradient by a reverse traversal of the graph. As the software runs the code to compute the function and its derivative, it records operations in a data structure called a trace\n",
    "\n",
    "### How do you want to handle cases for $ f: R^m -> R$ or later $f : R^m -> R^n$ ? Would it make sense to design a high-level function object to model arbitrary functions ff? You could think further and possibly plan for a grad() method or similar in a class that models ff, since computing the gradient (or Jacobian) is an operation that is often required.\n",
    "\n",
    "In `fabGraph` class, we have `compute_jacobian` method, which returns a 2-d matrix of partial derivatives.\n",
    "\n",
    "### Do you want/need to depend on other libraries? (e.g. NumPy)\n",
    "\n",
    "Yes. Certain operations like matrix multiplication are very well optimized in `numpy`, `scipy`, and `math`. We intend to use them rather than re-inventing them from scratch.\n",
    "\n",
    "\n",
    "\n",
    "## 6. License\n",
    "\n",
    "Licensing is essential for a new software, and every software must be licensed to protect its contributors and users. For the purpose of this project, we would be using the MIT License. \n",
    "\n",
    "### MIT License\n",
    "Originated in the late 1980s at the Massachusetts Institute of Technology (MIT) - the MIT License is a free permissive software license. It has high license compatibility as being a permissive license, it holds minimal restrictions on reuse. It gives the flexibility to allow the code to be reused for any purpose - as long as the original copy of the MIT License is included by the users in their distribution. \n",
    "\n",
    "The motivation to choose the ‘MIT License’ is as follows: \n",
    "\n",
    "\n",
    "*   Permissive \n",
    "\n",
    "    It allows unrestricted usage and free distribution and modification for any intended use of software.\n",
    "*   Easy to understand\n",
    "\n",
    "    The terms and conditions are straightforward  - making it easy for the user to understand and allowing them to modify, distribute or merge its copy with new copies. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e700b47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
